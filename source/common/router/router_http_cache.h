#pragma once

#include <cstddef>
#include <list>
#include <string>
#include <utility>
#include <vector>

#include "envoy/buffer/buffer.h"
#include "envoy/common/platform.h"
#include "envoy/event/deferred_deletable.h"
#include "envoy/event/schedulable_cb.h"
#include "envoy/http/async_client.h"
#include "envoy/http/filter.h"
#include "envoy/http/header_map.h"

#include "source/common/singleton/threadsafe_singleton.h"

#include "absl/synchronization/mutex.h"

namespace Envoy {
namespace Http {
namespace Cache {

class CacheEntry;
class SimpleCache;

class CacheServedRequest: public Event::DeferredDeletable {
public:
  CacheServedRequest(CacheEntry* parent, StreamDecoderFilterCallbacks* callbacks, bool head_request);
  // cancels scheduled event and removes it from entry's event list
  // gets called during router onDestroy()
  ~CacheServedRequest() override;
  // schedules schedulable_event_ for execution if do_schedule_
  void schedule();
  // sends headers and data available in cache
  void serve();

private:
  friend class CacheEntry;
  friend class SimpleCache;

  // stores an iterator for fast deletion from list of requests in an entry
  std::list<CacheServedRequest*>::iterator event_it_;
  // buffer to send data downstream
  Buffer::InstancePtr buffer_;
  // an event to schedule when schedule() is called, calls serve() on execution
  Event::SchedulableCallbackPtr schedulable_event_;
  // callbacks for the request
  StreamDecoderFilterCallbacks* callbacks_;
  // the entry to which this request belongs, nullptr if not schedulable
  CacheEntry* parent_;
  // how many fragments of data have been sent
  // improptu -1 means headers have not been sent, 0 means headers sent but no data
  int start_ = -1;
  // whether the request has HEAD as a method
  bool head_request_;
  // whether truly schedule the request when schedule() is called
  bool do_schedule_ = true;
};

class CacheEntry: public AsyncClient::StreamCallbacks {
public:
  ~CacheEntry() override;
  // compares incoming headers with stored cache key
  bool isSame(RequestHeaderMap& headers);
  // returns true if request can be handled by the cache (or is assumed to)
  bool isCacheable();
  // adds available data to buffer and calls callbacks' encodeHeader() and encodeData()
  bool reply(StreamDecoderFilterCallbacks* callbacks, Buffer::Instance& buffer, int& start, bool head_request);
  // adds a new request to the request list before the full response is available
  void addCallbackToDispatcher(StreamDecoderFilterCallbacks* callbacks, bool head_request, Event::DeferredDeletablePtr& dest);
  // assigns the cache key for this entry
  void assign(RequestHeaderMap& headers);
  // starts a new request for this entry that gets received by the cache
  void startRequest(RequestHeaderMap& headers, AsyncClient& client);
  // schedules requests in the waiting list for execution in the current iteration
  void scheduleCallbacks();

  // AsyncClient::StreamCallbacks 

  // cache incoming headers
  void onHeaders(ResponseHeaderMapPtr&& headers, bool end_stream) override;
  // cache incoming data
  void onData(Buffer::Instance& data, bool end_stream) override;
  // cache (or not) incoming trailers
  void onTrailers(ResponseTrailerMapPtr&&) override {} // caches may discard trailer fields
                                                       // though trailers could contain cache directives
  // sets cache entry as completed and calls scheduleCallbacks()
  void onComplete() override;
  // handle upstream connection issues, for now just use envoy's generated response and set as complete
  // should probably schedule a retry instead after some time (preferably configurable)
  void onReset() override { onComplete(); }

private:
  friend class CacheServedRequest;
  friend class SimpleCache;

  // mutex used for all operations that are not thread-safe
  // should be split into multiple mutexes to reduce blocking time
  absl::Mutex mtx_;
  // stores requests incoming during the herd
  std::list<CacheServedRequest*> events_;
  // functions as a simple list to allow multiple different entries for the same hash
  CacheEntry* next_ = nullptr;
  // used to store a pointer to a stream for a request generated by the cache
  AsyncClient::Stream* stream_ = nullptr;

  // storage and handles for entry's headers and data
  ResponseHeaderMapPtr response_headers_;
  RequestHeaderMapPtr cache_request_headers_;
  std::list<std::vector<char>> data_;
  int data_num_ = 0;

  // improptu storage for cache key, should be more adaptible, e.g. for vary headers
  std::string host_;
  std::string path_;

  // some basic entry states
  bool is_completed_ = false;
  bool cacheable_ = true;
  bool is_empty_ = true;
}; // some reordering of class members could be done to possibly reduce the size of this object

class SimpleCache {
public:
  ~SimpleCache();
  // called by server.cc to initialize the cache size based on config
  void init(std::size_t size);
  // called by router.cc during decodeHeaders to check if the cache can handle this request
  bool search(RequestHeaderMap& headers, StreamDecoderFilterCallbacks* callbacks, Event::DeferredDeletablePtr& dest, AsyncClient& client);
  
private:
  // returns a hash for the current request headers so we can assign it an entry
  std::size_t getHash(RequestHeaderMap& headers);

  // stores entries
  std::vector<CacheEntry*> cache_;
};

// global cache object
using Cache = ThreadSafeSingleton<SimpleCache>;

} // namespace Cache
} // namespace Http
} // namespace Envoy